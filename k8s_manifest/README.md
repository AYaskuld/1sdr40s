Допустим облачный провайер - AWS.  
Ссылка на <a href="https://github.com/AYaskuld/1sdr40s/blob/1c6998d86991ef51133e8ff14e9bb84979c4079c/k8s_manifest/deployment.yml">deplyment.yml</a> файл.
1) 5 нод, из них 1 мастер нода 4 - рабочих. как минимум 3 ноды уже находятся в разных зонах.
Т.к. ноды уже находятся в разных зонах, для достижения высокой доступности, мне достаточно разнести поды по разным нодам или назначить нодам метки "zone" и назначать поды уже непосредственно в зоны.
Код ниже запускает поды в разных зонах, при этом если одна нода не будет доступна, то под запустится на другой ноде. Это, конечно, уменьшит отказоустойчивость, но это позволит обработатывать пиковые нагрузки при отсутствии одной из нод.
```bash
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:          
              labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - web
              topologyKey: topology.kubernetes.io/zone
```
2)  Максимальное количество подов для автомаштабирования 4 поды, исходя из результатов нагрузочного теста.

3) Время для инициализации 5-10 секунд, проверку буду осуществлять с помощью readinessProbe, sturtupProbe и livnesProbe
Код ниже отправляет GET запрос на Ip адрес пода на 80 порт, если вернется ответ со статутом 200-399, то под будет считаться healthy.

```bash
startupProbe:
  httpGet:
    path: /
    port: 80
  initialDelaySeconds: 5
  failureThreshold: 3
  periodSeconds: 10
```

4) Для точного конфигурирования ресурсов, необходимо понять какое максимальное значение CPU необходимо для первых запросов.
Минимальные системные требования для ноды - 2 ядра и 2gb RAM. Если это единственное приложение, которое будет работать на ноде, то этого хватит.

 - Задам гарантируемый объем для CPU: 0.1 CPU = 100m. Дам возможность поду вертикально масштабироваться до 80% от общего CPU.    
 - Задам гарантируемый объем памяти 128Mi с возможностью масштабирования, ограничивать лимитами не буду т.к считаю, что на ноде запущено одно приложение.

5) При уменьшении или увеличении нагрузки на CPU в ночное время HPA будет сокращать ненужные поды, но для достижения отказоустойчивости необходимо как минимум две реплики, поэтому будет функционировать минимум 2 пода. 

Как результат - приложение установлено на 3 реплики и поддерживает 2 реплики в минимальном состоянии, и мы можем подняться до 4-х реплик для обработки пиковой нагрузки, начальное значение HPA было установлено исходя из режима покоя приложения ночью. Кластер будет отказоустойчивым и всегда будет готов начать работу.
