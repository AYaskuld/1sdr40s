
1) 5 нод, из них 1 мастер нода 4 - рабочих. как минимум 3 ноды уже находятся в разных зонах.
Т.к. ноды уже находятся в разных зонах, для достижения высокой доступности, мне достаточно разнести поды по разным нодам. 
Код ниже запускает поды на разных нодах.
```bash
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - web
              topologyKey: "kubernetes.io/hostname" 
```
2)  максимальное количество подов для автомаштабирования 4 поды, исходя из результатов нагрузочного теста.
3) время для инициализации 5-10 секунд, проверку буду осуществлять с помощью sturtupProbe и livnesProbe
Код ниже отправляет GET запрос на Ip адрес пода на 80 порт, если вернется ответ со статутом 200-399, то под будет считаться healthy.
```bash
startupProbe:
  httpGet:
    path: /
    port: 80
  initialDelaySeconds: 5
  failureThreshold: 40
  periodSeconds: 1
```

4) Для точного конфигурирования ресурсов, необходимо понять какое максимальное значение CPU необходимо для первых запросов. В этом примере задам гарантируемый объем cpu 100m с возможностью вертикального масштабирования до 1000m.  
Из условия объем памяти "районе 128Mi", задам гарантируемый объем памяти 128Mi с возможностью масштабирования до 256Mi, что бы при достижении подом значения в 128Mi под продолжал функционировать в то время как будет создаваться новый под.

5) При уменьшении запросов и нагрузки в ночное время HPA будет сокращать ненужные поды, но для достижения отказоустойчивости необходимо как минимум две реплики, поэтому в ночное время будет функционировать 2 пода.




