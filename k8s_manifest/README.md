
1) 5 нод, из них 1 мастер нода 4 - рабочих. как минимум 3 ноды уже находятся в разных зонах.
Т.к. ноды уже находятся в разных зонах, для достижения высокой доступности, мне достаточно разнести поды по разным нодам. 
Код ниже запускает поды на разных нодах.
```bash
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - web
              topologyKey: "kubernetes.io/hostname" 
```
2)  максимальное количество подов для автомаштабирования 4 поды, исходя из результатов нагрузочного теста.

3) время для инициализации 5-10 секунд, проверку буду осуществлять с помощью sturtupProbe и livnesProbe
Код ниже отправляет GET запрос на Ip адрес пода на 80 порт, если вернется ответ со статутом 200-399, то под будет считаться healthy.
```bash
startupProbe:
  httpGet:
    path: /
    port: 80
  initialDelaySeconds: 5
  failureThreshold: 40
  periodSeconds: 1
```

4) Для точного конфигурирования ресурсов, необходимо понять какое максимальное значение CPU необходимо для первых запросов.
Минимальные системные требования для ноды - 2 ядра и 2gb RAM. Если это единственное приложение, которое будет работать на ноде, то этого хватит.

Задам гарантируемый объем: 0.1 CPU = 100m. Дам возможность поду вертикально масштабироваться до 80% от общего CPU.    
Задам гарантируемый объем памяти 128Mi с возможностью масштабирования, ограничивать лимитами не буду т.к считаю, что на ноде запущено одно приложение.

5) При уменьшении или увеличении нагрузки на CPU в ночное время HPA будет сокращать ненужные поды, но для достижения отказоустойчивости необходимо как минимум две реплики, поэтому будет функционировать минимум 2 пода. 



